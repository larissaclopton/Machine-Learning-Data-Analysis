{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "spark = SparkSession.builder.master(\"local[10]\").appName(\"beer_review\").getOrCreate()\n",
    "df = spark.read.json(\"/project/cmsc25025/beer_review/labeled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing example mean, median, and standard deviation for beers\n",
      "[[11.0, 10.0, 2.1602468994692869], [15.935153583617748, 16.0, 1.8230304422792614], [13.0, 13.0, 1.0], [15.621621621621621, 16.0, 1.5130308110571249], [12.5, 12.5, 1.5]]\n",
      "Showing example mean, median, and standard deviation for brewers\n",
      "[[12.836223506743737, 13.0, 2.5432971325766625], [15.414285714285715, 16.0, 2.4985301801708322], [11.348958333333334, 12.0, 2.7590946126577864], [7.0, 6.0, 3.7416573867739413], [14.141904761904762, 14.0, 2.2235084939517855]]\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - data inspection\n",
    "\n",
    "# get mean, median and standard deviation of the overall ratings for each beer and brewer\n",
    "beer_ratings = df.rdd.map(lambda x: (int(x['beer_id']),[int(x['overall'])])).reduceByKey(lambda a,b: a + b)\n",
    "beer_stats = beer_ratings.map(lambda x: [[np.mean(x[1]),np.median(x[1]),np.std(x[1])]]).reduce(lambda a,b: a + b)\n",
    "brewer_ratings = df.rdd.map(lambda x: (int(x['brewer']),[int(x['overall'])])).reduceByKey(lambda a,b: a + b)\n",
    "brewer_stats = brewer_ratings.map(lambda x: [[np.mean(x[1]),np.median(x[1]),np.std(x[1])]]).reduce(lambda a,b: a + b)\n",
    "\n",
    "print \"Showing example mean, median, and standard deviation for beers\"\n",
    "print beer_stats[:5]\n",
    "print \"Showing example mean, median, and standard deviation for brewers\"\n",
    "print brewer_stats[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given beer, the mean and median are similar and the standard deviation is generally small, so overall people have similar taste. In comparison, for a given brewer (across all of their beers), the mean and median are farther apart and the standard deviation is larger. Therefore, people do not have as similar opinions of brewers overall as they do for specific beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, csr_matrix, vstack\n",
    "import json\n",
    "import string\n",
    "import math\n",
    "\n",
    "# Part 2 - sentiment analysis (positive or negative)\n",
    "\n",
    "# load the vocabulary\n",
    "with open(\"/project/cmsc25025/beer_review/vocab_50.json\", \"r\") as f:\n",
    "         d = json.load(f)\n",
    "vocab = d.keys()\n",
    "\n",
    "def getFeatures(review):\n",
    "    # normalize text and separate by spaces\n",
    "    words = review.lower().encode('utf-8').translate(string.maketrans(\"\",\"\"), \\\n",
    "                                                     string.punctuation).split()\n",
    "    # obtain word_ids for vocab words in the review\n",
    "    word_ids = []\n",
    "    for i,word in enumerate(words):\n",
    "        if d.has_key(word):\n",
    "            word_ids.append(d[word])\n",
    "    vals = [1]*len(word_ids)\n",
    "    rows = [0]*len(vals)\n",
    "    \n",
    "    # create a sparse matrix feature vector\n",
    "    result = coo_matrix((vals, (rows,word_ids)), shape=(1,len(vocab))).tocsr()[0]\n",
    "    return result\n",
    "    # TODO - drop review if none of the words are in the dictionary        \n",
    "                              \n",
    "def predict(beta,x,y):\n",
    "    \n",
    "    prob = 1/(1+np.exp(-x.dot(beta)[0]))\n",
    "    yhat = (1 if prob >= 0.5 else 0)\n",
    "    return y != yhat\n",
    "    \n",
    "def updateBeta(beta,X,error,etta,N,lam):\n",
    "    \n",
    "    trans = X.transpose()\n",
    "    rdd = spark.sparkContext.parallelize([x for x in trans])\n",
    "    grad = np.array(rdd.map(lambda x: x.multiply(error).sum()/float(N)).collect())\n",
    "    return beta + etta*(grad - lam)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train a model based on text reviews\n",
    "def train_model_text(train_data,test_data,test_size,epochs,lam):\n",
    "\n",
    "    # split data into batches\n",
    "    num_batch = 1000\n",
    "    batch_split = [1]*num_batch\n",
    "    batches = train_data.randomSplit(batch_split)\n",
    "    error = [0]*(num_batch*epochs) # error rate\n",
    "    neg_ll = [0]*(num_batch*epochs) # negative log likelihood\n",
    "    \n",
    "    # initialize beta\n",
    "    beta = np.zeros(len(vocab))\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        print \"epoch %d\" % ep\n",
    "        # make a full pass over the training data\n",
    "        for idx,batch in enumerate(batches):\n",
    "            \n",
    "            etta = c1/(math.pow(ep*num_batch+idx+1,c2)) # learning rate\n",
    "            features = batch.map(lambda x: getFeatures(x['review'])).reduce(lambda a,b: vstack([a,b]))\n",
    "            score = np.array(batch.map(lambda x: [int(x['overall'] >= 14)]).reduce(lambda a,b: a + b))                \n",
    "            batch_size = len(score)\n",
    "    \n",
    "            # update betas\n",
    "            feat = spark.sparkContext.parallelize([x for x in features])\n",
    "            pi = np.array(feat.map(lambda x: [1/(1+np.exp(-x.dot(beta)[0]))]).collect())\n",
    "            diff = score - pi\n",
    "            beta = updateBeta(beta,features,diff,etta,batch_size,lam)\n",
    "\n",
    "            # compute the negative log likelihood\n",
    "            combined = [(score[k],pi[k]) for k in range(batch_size)]\n",
    "            rdd = spark.sparkContext.parallelize(combined)\n",
    "            neg_ll[ep*num_batch+idx] = -rdd.map(lambda x: x[0]*math.log(x[1]) + (1-x[0])*math.log(1-x[1])).sum()\n",
    "            #print neg_ll[ep*num_batch+idx]\n",
    "            \n",
    "            # compute the error rate on the test set\n",
    "            error[ep*num_batch+idx] = (test_data.map(lambda x: predict(beta,getFeatures(x['review']),int(x['overall'] >= 14))).sum())/float(test_size)\n",
    "            print error[ep*num_batch+idx]\n",
    "\n",
    "    return error, neg_ll     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0.351951014399\n",
      "0.344726229275\n",
      "0.334787783692\n",
      "0.332149353423\n",
      "0.330706545833\n",
      "0.330155753363\n",
      "0.329258364657\n",
      "0.328785489171\n",
      "0.328565172183\n",
      "0.328202455191\n",
      "0.327804809896\n",
      "0.327546877813\n",
      "0.327385670261\n",
      "0.327272824974\n",
      "0.327122364592\n",
      "0.326990711758\n",
      "0.326832190999\n",
      "0.326740840053\n",
      "0.326646802314\n",
      "0.326595753256\n",
      "0.326542017405\n",
      "0.326490968347\n",
      "0.326480221177\n",
      "0.326421111741\n",
      "0.326434545704\n",
      "0.326353941928\n",
      "0.326297519284\n",
      "0.326238409849\n",
      "0.326230349471\n",
      "0.326179300413\n",
      "0.326149745695\n",
      "0.326130938147\n",
      "0.326074515504\n",
      "0.326050334371\n",
      "0.326010032483\n",
      "0.325985851351\n",
      "0.325958983425\n",
      "0.325875692857\n",
      "0.325875692857\n",
      "0.325889126819\n",
      "0.325881066442\n",
      "0.325867632479\n",
      "0.325838077761\n",
      "0.325778968325\n",
      "0.3257521004\n",
      "0.325757473985\n",
      "0.32577359474\n",
      "0.325746726815\n",
      "0.325711798512\n",
      "0.325695677757\n",
      "0.325684930587\n",
      "0.325660749454\n",
      "0.325658062661\n",
      "0.325655375869\n",
      "0.325658062661\n",
      "0.325647315491\n",
      "0.325644628699\n",
      "0.325650002284\n",
      "0.325631194736\n",
      "0.325617760773\n",
      "0.325593579641\n",
      "0.325593579641\n",
      "0.32558283247\n",
      "0.325588206055\n",
      "0.32558283247\n",
      "0.325566711715\n",
      "0.325547904167\n",
      "0.325542530582\n",
      "0.325547904167\n",
      "0.325526409827\n",
      "0.325523723035\n",
      "0.32552909662\n",
      "0.325521036242\n",
      "0.325521036242\n",
      "0.32551834945\n",
      "0.325537156997\n",
      "0.325534470205\n",
      "0.325531783412\n",
      "0.325526409827\n",
      "0.325534470205\n",
      "0.325526409827\n",
      "0.325537156997\n",
      "0.325531783412\n",
      "0.325523723035\n",
      "0.325526409827\n",
      "0.325515662657\n",
      "0.325521036242\n",
      "0.325510289072\n",
      "0.325496855109\n",
      "0.325494168317\n",
      "0.325486107939\n",
      "0.325486107939\n",
      "0.325488794732\n",
      "0.325494168317\n",
      "0.325478047562\n",
      "0.325469987184\n",
      "0.325467300391\n",
      "0.325456553221\n",
      "0.325461926806\n",
      "0.325453866429\n",
      "0.325443119259\n",
      "0.325453866429\n",
      "0.325445806051\n",
      "0.325435058881\n",
      "0.325432372089\n",
      "0.325435058881\n",
      "0.325435058881\n",
      "0.325437745674\n",
      "0.325435058881\n",
      "0.325448492844\n",
      "0.325448492844\n",
      "0.325448492844\n",
      "0.325445806051\n",
      "0.325440432466\n",
      "0.325437745674\n",
      "0.325435058881\n",
      "0.325435058881\n",
      "0.325435058881\n",
      "0.325429685296\n",
      "0.325429685296\n",
      "0.325418938126\n",
      "0.325424311711\n",
      "0.325421624918\n",
      "0.325416251333\n",
      "0.325413564541\n",
      "0.325416251333\n",
      "0.325418938126\n",
      "0.325418938126\n",
      "0.325416251333\n",
      "0.325437745674\n",
      "0.325432372089\n",
      "0.325418938126\n",
      "0.325416251333\n",
      "0.325416251333\n",
      "0.325410877748\n",
      "0.325402817371\n",
      "0.325410877748\n",
      "0.325408190956\n",
      "0.325408190956\n",
      "0.325405504163\n",
      "0.325397443786\n",
      "0.325386696615\n",
      "0.325389383408\n",
      "0.325384009823\n",
      "0.325386696615\n",
      "0.325389383408\n",
      "0.325392070201\n",
      "0.325392070201\n",
      "0.325378636238\n",
      "0.325384009823\n",
      "0.32538132303\n",
      "0.325375949445\n",
      "0.325375949445\n",
      "0.325375949445\n",
      "0.325375949445\n",
      "0.32535982869\n",
      "0.325354455105\n",
      "0.325362515483\n",
      "0.325357141898\n",
      "0.32535982869\n",
      "0.325357141898\n",
      "0.325354455105\n",
      "0.325357141898\n",
      "0.325354455105\n",
      "0.325357141898\n",
      "0.325351768313\n",
      "0.325357141898\n",
      "0.325346394727\n",
      "0.325341021142\n",
      "0.32533833435\n",
      "0.32533833435\n",
      "0.325341021142\n",
      "0.325335647557\n",
      "0.32533833435\n",
      "0.325341021142\n",
      "0.325341021142\n",
      "0.325341021142\n",
      "0.325341021142\n",
      "0.325330273972\n",
      "0.32533833435\n",
      "0.32534908152\n",
      "0.32534908152\n",
      "0.325357141898\n",
      "0.325343707935\n",
      "0.32534908152\n",
      "0.32534908152\n",
      "0.325341021142\n",
      "0.325343707935\n",
      "0.325343707935\n",
      "0.325343707935\n",
      "0.325341021142\n",
      "0.325335647557\n",
      "0.325335647557\n",
      "0.32533833435\n",
      "0.325335647557\n",
      "0.325332960765\n",
      "0.325330273972\n",
      "0.325335647557\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.32533833435\n",
      "0.32533833435\n",
      "0.325341021142\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.325332960765\n",
      "0.325324900387\n",
      "0.325324900387\n",
      "0.325330273972\n",
      "0.325335647557\n",
      "0.325332960765\n",
      "0.325335647557\n",
      "0.32532758718\n",
      "0.32532758718\n",
      "0.32532758718\n",
      "0.32532758718\n",
      "0.325330273972\n",
      "0.325330273972\n",
      "0.325330273972\n",
      "0.32532758718\n",
      "0.32532758718\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.32532758718\n",
      "0.325324900387\n",
      "0.325330273972\n",
      "0.325324900387\n",
      "0.325324900387\n",
      "0.325322213595\n",
      "0.325324900387\n",
      "0.325322213595\n",
      "0.325324900387\n",
      "0.325322213595\n",
      "0.32532758718\n",
      "0.325324900387\n",
      "0.325324900387\n",
      "0.325324900387\n",
      "0.325324900387\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325319526802\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.32531684001\n",
      "0.32531684001\n",
      "0.325322213595\n",
      "0.325319526802\n",
      "0.325322213595\n",
      "0.325322213595\n",
      "0.325319526802\n",
      "0.325322213595\n",
      "0.325319526802\n",
      "0.32531684001\n",
      "0.325314153217\n",
      "0.32531684001\n",
      "0.32531684001\n",
      "0.32531684001\n",
      "0.32531684001\n",
      "0.325314153217\n",
      "0.32531684001\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325311466424\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325314153217\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325314153217\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325308779632\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325306092839\n",
      "0.325308779632\n",
      "0.325311466424\n",
      "0.325308779632\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325300719254\n",
      "0.325300719254\n",
      "0.325303406047\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325308779632\n",
      "0.325311466424\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325311466424\n",
      "0.325311466424\n",
      "0.325306092839\n",
      "0.325303406047\n",
      "0.325308779632\n",
      "0.325303406047\n",
      "0.325303406047\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325308779632\n",
      "0.325306092839\n",
      "0.325306092839\n",
      "0.325303406047\n",
      "0.325300719254\n",
      "0.325303406047\n",
      "0.325303406047\n",
      "0.325303406047\n",
      "0.325303406047\n",
      "0.325300719254\n",
      "0.325300719254\n",
      "0.325303406047\n",
      "0.325300719254\n",
      "0.325298032462\n",
      "0.325298032462\n",
      "0.325298032462\n",
      "0.325298032462\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325298032462\n",
      "0.325300719254\n",
      "0.325300719254\n",
      "0.325300719254\n",
      "0.325298032462\n",
      "0.325298032462\n",
      "0.325298032462\n",
      "0.325292658877\n",
      "0.325298032462\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325295345669\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325289972084\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325292658877\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325289972084\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325284598499\n",
      "0.325287285292\n",
      "0.325284598499\n",
      "0.325287285292\n",
      "0.325284598499\n",
      "0.325284598499\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325289972084\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325287285292\n",
      "0.325284598499\n",
      "0.325281911707\n",
      "0.325281911707\n",
      "0.325281911707\n",
      "0.325281911707\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325276538122\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325279224914\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325273851329\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325276538122\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325276538122\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325273851329\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325271164536\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325271164536\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325268477744\n",
      "0.325268477744\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325265790951\n",
      "0.325260417366\n",
      "0.325265790951\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325265790951\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325263104159\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325257730574\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n",
      "0.325260417366\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d8aebaf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning rate parameters\n",
    "c1 = 1 # learning rate parameters\n",
    "c2 = 2\n",
    "\n",
    "# obtain data sets\n",
    "train, test, validate = df.rdd.randomSplit([0.7,0.15,0.15])\n",
    "test_size = test.count()\n",
    "validate_size = validate.count()\n",
    "    \n",
    "# select lam using validation set\n",
    "#lam = [0.2,0.5,2,10] # these can be changed\n",
    "#epochs = 1 # this can be changed\n",
    "#for l in lam:\n",
    "#    train_model_text(train,validate,validate_size,epochs,l)\n",
    "\n",
    "# train model on both train and validate, using optimized lam\n",
    "epochs = 1\n",
    "best_lam = 0.5\n",
    "training = train.union(validate)\n",
    "error, neg_ll = train_model_text(training,test,test_size,epochs,best_lam)\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('Error Rate versus Iteration')\n",
    "plt.show()\n",
    "    \n",
    "#plt.figure()\n",
    "#plt.plot(neg_ll)\n",
    "#plt.xlabel('iteration')\n",
    "#plt.ylabel('negative log likelihood')\n",
    "#plt.title('Negative Log Likelihood versus Iteration')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the error rates after each iteration. At the end of the output you will find a plot of error rate versus iteration and find that it fairly steadily decreases.\n",
    "\n",
    "We found that a lambda of 0.5 got us the best values for the our error rate. We also found that a step size where we set c2 equal to 2 and c1 equal to 1 got us the best values. Our error rate using the text is about 32.5%. It levels off fairly quickly, possibly because the step size became small as the number of iterations increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.464779772929\n",
      "0.531137772768\n",
      "0.415250825348\n",
      "0.369082320101\n",
      "0.464463053923\n",
      "0.435370534396\n",
      "0.418058351451\n",
      "0.394240008589\n",
      "0.414405346646\n",
      "0.408787610382\n",
      "0.401849316907\n",
      "0.414429503181\n",
      "0.380248007086\n",
      "0.381316262716\n",
      "0.374834259334\n",
      "0.376294387632\n",
      "0.376892932872\n",
      "0.374839627453\n",
      "0.375974984567\n",
      "0.374877204284\n",
      "0.373468073114\n",
      "0.374222293797\n",
      "0.373113777277\n",
      "0.371543602544\n",
      "0.371457712645\n",
      "0.370314303352\n",
      "0.372434710256\n",
      "0.372268298575\n",
      "0.372128727487\n",
      "0.369117212873\n",
      "0.368612609711\n",
      "0.367914754274\n",
      "0.366672034785\n",
      "0.368354940011\n",
      "0.368652870601\n",
      "0.367710765762\n",
      "0.368974957726\n",
      "0.367469200419\n",
      "0.367541670022\n",
      "0.364758300454\n",
      "0.36729473656\n",
      "0.367098800225\n",
      "0.367407467053\n",
      "0.367380626459\n",
      "0.367530933784\n",
      "0.367726870118\n",
      "0.367402098934\n",
      "0.366854550823\n",
      "0.366808921813\n",
      "0.367748342593\n",
      "0.367063907454\n",
      "0.366892127654\n",
      "0.367496041012\n",
      "0.367627559922\n",
      "0.3675497222\n",
      "0.367651716456\n",
      "0.366894811713\n",
      "0.366685455082\n",
      "0.366808921813\n",
      "0.367063907454\n",
      "0.366902863891\n",
      "0.367702713584\n",
      "0.367673188931\n",
      "0.366647878251\n",
      "0.367471884478\n",
      "0.367627559922\n",
      "0.367729554178\n",
      "0.36784765279\n",
      "0.367920122393\n",
      "0.367925490512\n",
      "0.367938910809\n",
      "0.367724186059\n",
      "0.36771881794\n",
      "0.366897495773\n",
      "0.367624875862\n",
      "0.367721502\n",
      "0.367726870118\n",
      "0.36771881794\n",
      "0.367627559922\n",
      "0.367708081703\n",
      "0.367721502\n",
      "0.367665136753\n",
      "0.367708081703\n",
      "0.367651716456\n",
      "0.367667820812\n",
      "0.36771881794\n",
      "0.367415519231\n",
      "0.367710765762\n",
      "0.367445043884\n",
      "0.367657084575\n",
      "0.367662452693\n",
      "0.367624875862\n",
      "0.367496041012\n",
      "0.366897495773\n",
      "0.367050487157\n",
      "0.36665056231\n",
      "0.366744504388\n",
      "0.367061223394\n",
      "0.367061223394\n",
      "0.367289368441\n",
      "0.367050487157\n",
      "0.366897495773\n",
      "0.367053171216\n",
      "0.367012910326\n",
      "0.366940440723\n",
      "0.366690823201\n",
      "0.366688139142\n",
      "0.366688139142\n",
      "0.366647878251\n",
      "0.367061223394\n",
      "0.366674718845\n",
      "0.36754703814\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.36754703814\n",
      "0.367541670022\n",
      "0.367474568537\n",
      "0.367552406259\n",
      "0.367496041012\n",
      "0.367654400515\n",
      "0.367708081703\n",
      "0.367705397643\n",
      "0.367724186059\n",
      "0.367651716456\n",
      "0.367651716456\n",
      "0.367670504872\n",
      "0.367665136753\n",
      "0.367670504872\n",
      "0.367651716456\n",
      "0.367530933784\n",
      "0.367471884478\n",
      "0.367646348337\n",
      "0.367702713584\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366688139142\n",
      "0.36669350726\n",
      "0.367061223394\n",
      "0.367063907454\n",
      "0.366685455082\n",
      "0.366658614489\n",
      "0.366672034785\n",
      "0.366744504388\n",
      "0.366943124782\n",
      "0.366886759535\n",
      "0.367053171216\n",
      "0.366808921813\n",
      "0.366964597257\n",
      "0.366744504388\n",
      "0.366612985479\n",
      "0.367066591513\n",
      "0.367053171216\n",
      "0.366685455082\n",
      "0.366623721717\n",
      "0.366948492901\n",
      "0.366921652307\n",
      "0.367066591513\n",
      "0.366961913198\n",
      "0.366964597257\n",
      "0.366940440723\n",
      "0.366956545079\n",
      "0.366940440723\n",
      "0.367061223394\n",
      "0.367053171216\n",
      "0.366615669539\n",
      "0.366623721717\n",
      "0.366905547951\n",
      "0.367466516359\n",
      "0.367474568537\n",
      "0.367541670022\n",
      "0.367651716456\n",
      "0.367627559922\n",
      "0.367474568537\n",
      "0.367530933784\n",
      "0.367479936656\n",
      "0.367474568537\n",
      "0.367415519231\n",
      "0.366902863891\n",
      "0.367557774378\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.367541670022\n",
      "0.367662452693\n",
      "0.366902863891\n",
      "0.366886759535\n",
      "0.367541670022\n",
      "0.366886759535\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.36665056231\n",
      "0.366615669539\n",
      "0.366623721717\n",
      "0.366623721717\n",
      "0.366623721717\n",
      "0.366672034785\n",
      "0.366663982607\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.36673913627\n",
      "0.366741820329\n",
      "0.366808921813\n",
      "0.366741820329\n",
      "0.366672034785\n",
      "0.366645194192\n",
      "0.366645194192\n",
      "0.366658614489\n",
      "0.366647878251\n",
      "0.366669350726\n",
      "0.366645194192\n",
      "0.366685455082\n",
      "0.366623721717\n",
      "0.366623721717\n",
      "0.366661298548\n",
      "0.366672034785\n",
      "0.366647878251\n",
      "0.366690823201\n",
      "0.366612985479\n",
      "0.366661298548\n",
      "0.366661298548\n",
      "0.366647878251\n",
      "0.366672034785\n",
      "0.366806237754\n",
      "0.366741820329\n",
      "0.366647878251\n",
      "0.366672034785\n",
      "0.366808921813\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366996805969\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.36754703814\n",
      "0.367541670022\n",
      "0.367552406259\n",
      "0.367541670022\n",
      "0.367552406259\n",
      "0.367541670022\n",
      "0.367555090319\n",
      "0.367541670022\n",
      "0.367555090319\n",
      "0.367651716456\n",
      "0.367662452693\n",
      "0.367557774378\n",
      "0.367423571409\n",
      "0.367651716456\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.366905547951\n",
      "0.366663982607\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.36665056231\n",
      "0.366669350726\n",
      "0.366663982607\n",
      "0.366806237754\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366806237754\n",
      "0.366905547951\n",
      "0.367541670022\n",
      "0.36754703814\n",
      "0.367541670022\n",
      "0.367552406259\n",
      "0.367627559922\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367555090319\n",
      "0.367557774378\n",
      "0.36754703814\n",
      "0.367541670022\n",
      "0.3675497222\n",
      "0.367541670022\n",
      "0.367552406259\n",
      "0.367651716456\n",
      "0.367552406259\n",
      "0.367415519231\n",
      "0.367423571409\n",
      "0.367466516359\n",
      "0.367627559922\n",
      "0.367479936656\n",
      "0.367471884478\n",
      "0.367493356953\n",
      "0.367493356953\n",
      "0.367493356953\n",
      "0.367536301903\n",
      "0.367536301903\n",
      "0.367493356953\n",
      "0.36750677725\n",
      "0.367493356953\n",
      "0.367485304775\n",
      "0.367496041012\n",
      "0.367496041012\n",
      "0.36750677725\n",
      "0.367471884478\n",
      "0.367479936656\n",
      "0.367471884478\n",
      "0.367479936656\n",
      "0.367474568537\n",
      "0.367496041012\n",
      "0.36750677725\n",
      "0.367493356953\n",
      "0.36750677725\n",
      "0.367496041012\n",
      "0.367471884478\n",
      "0.367471884478\n",
      "0.367471884478\n",
      "0.367474568537\n",
      "0.367627559922\n",
      "0.367415519231\n",
      "0.367654400515\n",
      "0.367651716456\n",
      "0.367541670022\n",
      "0.367477252597\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366996805969\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366897495773\n",
      "0.366902863891\n",
      "0.366806237754\n",
      "0.366672034785\n",
      "0.366663982607\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.36665056231\n",
      "0.366623721717\n",
      "0.366661298548\n",
      "0.366645194192\n",
      "0.366645194192\n",
      "0.366647878251\n",
      "0.366661298548\n",
      "0.366647878251\n",
      "0.36665056231\n",
      "0.36665324637\n",
      "0.366661298548\n",
      "0.366658614489\n",
      "0.366612985479\n",
      "0.366612985479\n",
      "0.366688139142\n",
      "0.366690823201\n",
      "0.366690823201\n",
      "0.366690823201\n",
      "0.366623721717\n",
      "0.366688139142\n",
      "0.366615669539\n",
      "0.366612985479\n",
      "0.366658614489\n",
      "0.366658614489\n",
      "0.366642510132\n",
      "0.366658614489\n",
      "0.366661298548\n",
      "0.366612985479\n",
      "0.366658614489\n",
      "0.366658614489\n",
      "0.36665324637\n",
      "0.366647878251\n",
      "0.366666666667\n",
      "0.366647878251\n",
      "0.366661298548\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366744504388\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366886759535\n",
      "0.366996805969\n",
      "0.367477252597\n",
      "0.36690823201\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.367493356953\n",
      "0.367541670022\n",
      "0.366996805969\n",
      "0.366902863891\n",
      "0.366996805969\n",
      "0.367541670022\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366741820329\n",
      "0.366663982607\n",
      "0.366741820329\n",
      "0.366663982607\n",
      "0.36673913627\n",
      "0.366741820329\n",
      "0.366808921813\n",
      "0.366672034785\n",
      "0.36665324637\n",
      "0.366661298548\n",
      "0.366645194192\n",
      "0.366645194192\n",
      "0.366642510132\n",
      "0.366645194192\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366647878251\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366663982607\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366669350726\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366661298548\n",
      "0.366666666667\n",
      "0.366647878251\n",
      "0.366672034785\n",
      "0.366666666667\n",
      "0.366672034785\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366741820329\n",
      "0.366808921813\n",
      "0.366741820329\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366897495773\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366897495773\n",
      "0.366808921813\n",
      "0.366806237754\n",
      "0.366741820329\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366741820329\n",
      "0.36673913627\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366741820329\n",
      "0.366808921813\n",
      "0.366808921813\n",
      "0.36673913627\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.366747188448\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366666666667\n",
      "0.36665056231\n",
      "0.366645194192\n",
      "0.366645194192\n",
      "0.36665324637\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.36665324637\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.366647878251\n",
      "0.36665324637\n",
      "0.36665324637\n",
      "0.366647878251\n",
      "0.366645194192\n",
      "0.366645194192\n",
      "0.36665324637\n",
      "0.366647878251\n",
      "0.366661298548\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366666666667\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366663982607\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366666666667\n",
      "0.366669350726\n",
      "0.366669350726\n",
      "0.366647878251\n",
      "0.366669350726\n",
      "0.366661298548\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366747188448\n",
      "0.366747188448\n",
      "0.366663982607\n",
      "0.366669350726\n",
      "0.366669350726\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366744504388\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366669350726\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366744504388\n",
      "0.366663982607\n",
      "0.366747188448\n",
      "0.366669350726\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366672034785\n",
      "0.366669350726\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366663982607\n",
      "0.366744504388\n",
      "0.36673913627\n",
      "0.366744504388\n",
      "0.366808921813\n",
      "0.366806237754\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.36690823201\n",
      "0.366905547951\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366806237754\n",
      "0.366905547951\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366897495773\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366741820329\n",
      "0.36673913627\n",
      "0.366747188448\n",
      "0.366808921813\n",
      "0.366808921813\n",
      "0.366897495773\n",
      "0.366902863891\n",
      "0.366806237754\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.36690823201\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.36690823201\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366900179832\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.366886759535\n",
      "0.366900179832\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366905547951\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366902863891\n",
      "0.366886759535\n",
      "0.367477252597\n",
      "0.367541670022\n",
      "0.367477252597\n",
      "0.366900179832\n",
      "0.366996805969\n",
      "0.366886759535\n",
      "0.366996805969\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.36754703814\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.367541670022\n",
      "0.36754703814\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.367555090319\n",
      "0.367555090319\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.367555090319\n",
      "0.367552406259\n",
      "0.367555090319\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.367552406259\n",
      "0.367552406259\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 3523 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ece29983291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mbest_lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_lam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9ece29983291>\u001b[0m in \u001b[0;36mtrain_model_scores\u001b[0;34m(train_data, test_data, test_size, epochs, lam)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# compute the error rate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'appearance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aroma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'palate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'taste'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetFeatures2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/spark-2.1-el7-x86_64/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/spark-2.1-el7-x86_64/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/spark-2.1-el7-x86_64/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python-2.7.12-el7-x86_64/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/spark-2.1-el7-x86_64/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python-2.7.12-el7-x86_64/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 3523 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# train a model based on attribute scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getFeatures2(row,col,data):\n",
    "    return csr_matrix((data, (row, col)), shape=(1, 5))[0]\n",
    "\n",
    "def train_model_scores(train_data,test_data,test_size,epochs,lam):\n",
    "\n",
    "    # split data into batches\n",
    "    num_batch = 10000\n",
    "    batch_split = [1]*num_batch\n",
    "    batches = train_data.randomSplit(batch_split)\n",
    "    error = [0]*(num_batch*epochs) # error rate\n",
    "    \n",
    "    # initialize beta\n",
    "    beta = np.zeros(5)\n",
    "\n",
    "    row = np.array([0]*5)\n",
    "    col = np.array([0,1,2,3,4])\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # make a full pass over the training data\n",
    "        for idx,batch in enumerate(batches):\n",
    "            \n",
    "            etta = c1/(math.pow(ep*num_batch+idx+1,c2)) # learning rate\n",
    "            \n",
    "            scores = batch.map(lambda x:  np.array([x['appearance'] , x['aroma'], x['palate'] , x['style'] ,  x['taste']]))\n",
    "            features = scores.map(lambda x: getFeatures2(row,col,x)).reduce(lambda a,b: vstack([a,b]))\n",
    "            \n",
    "            #features = batch.map(lambda x: getFeatures2(row,col,np.array(x['appearance'], x['aroma'], x['palate'], x['style'], x['taste']))).reduce(lambda a,b: vstack([a,b]))\n",
    "            score = np.array(batch.map(lambda x: [int(x['overall'] >= 14)]).reduce(lambda a,b: a + b))                \n",
    "            batch_size = len(score)\n",
    "    \n",
    "            # update betas\n",
    "            feat = spark.sparkContext.parallelize([x for x in features])\n",
    "            pi = np.array(feat.map(lambda x: [1/(1+np.exp(-x.dot(beta)[0]))]).collect())\n",
    "            diff = score - pi\n",
    "            beta = updateBeta(beta,features,diff,etta,batch_size,lam)\n",
    "\n",
    "            # compute the negative log likelihood\n",
    "            #combined = [(score[k],pi[k]) for k in range(batch_size)]\n",
    "            #rdd = spark.sparkContext.parallelize(combined)\n",
    "            #neg_ll[ep*num_batch+idx] = -rdd.map(lambda x: x[0]*math.log(x[1]) + (1-x[0])*math.log(1-x[1])).sum()\n",
    "            #print neg_ll[ep*num_batch+idx]\n",
    "            \n",
    "            # compute the error rate on the test set\n",
    "            test_scores = test_data.map(lambda x: (np.array([x['appearance'], x['aroma'], x['palate'], x['style'], x['taste']]), int(x['overall'] >= 14)))\n",
    "            error[ep*num_batch+idx] = test_scores.map(lambda x: predict(beta,getFeatures2(row,col, x[0]),x[1])).sum()/float(test_size)\n",
    "            print error[ep*num_batch+idx]\n",
    "    return error\n",
    "\n",
    "# learning rate parameters\n",
    "c1 = 1 # learning rate parameters\n",
    "c2 = 2\n",
    "\n",
    "# obtain data sets\n",
    "train, test, validate = df.rdd.randomSplit([0.7,0.15,0.15])\n",
    "test_size = test.count()\n",
    "validate_size = validate.count()\n",
    "    \n",
    "# select lam using validation set\n",
    "#lam = [0.2,0.5,2,10] # these can be changed\n",
    "#epochs = 1 # this can be changed\n",
    "#for l in lam:\n",
    "#    train_model_text(train,validate,validate_size,epochs,l)\n",
    "\n",
    "# train model on both train and validate, using optimized lam\n",
    "epochs = 1\n",
    "best_lam = 1\n",
    "training = train.union(validate)\n",
    "error, neg_ll = train_model_scores(training,test,test_size,epochs,best_lam)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('Error Rate versus Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above displays the error rate for each iteration. The prediction error is higher for the logistic regression based on the numerical consumer ratings than it is for the one with text review features. The error begins around 50% and bounces up and down before eventually leveling off around 36.67%. Therefore, the model based on text review features is better than the model based on numerical consumer ratings, implying that the feature vector representation for the text reviews is more powerful than raw numerical scores. This makes sense because the text is of a much higher dimension, containing a lot more info and having the ability to explain a lot more. People are able to use the text to say things they are not able to do just with ratings. In addition, certain types of ratings may not correlate well with the overall rating. We actually compute correlations below and found that style was very weakly related to the overall rating, and appearance was fairly weakly correlated.  \n",
    "\n",
    "We have generally found that we get the best tradeoff between performance and runtime when we set c_1 equal to 1 and c_2 equal to 2. When we made c_2 smaller it took a very long time to get closer to convergence and when we made c_2 bigger are data was much jumpier and was less clear that we were decreasing constantly. \n",
    "\n",
    "Here are some example error rates for the first few iterations using various parameters. \n",
    "\n",
    "lam = .5 \n",
    "0.533179372462\n",
    "0.462512941833\n",
    "0.476079737784\n",
    "0.462512941833\n",
    "0.52929548149\n",
    "0.462512941833\n",
    "0.533452961467\n",
    "0.462512941833\n",
    "0.536862095048\n",
    "0.462512941833...\n",
    "\n",
    "lam = 1\n",
    "0.462816251452\n",
    "0.537183748548\n",
    "0.462816251452\n",
    "0.537183748548\n",
    "0.462816251452\n",
    "0.531915580225\n",
    "0.462816251452\n",
    "0.533257497526\n",
    "0.462816251452...\n",
    "\n",
    "lam = 1 , c2 = 2\n",
    "0.529491011549\n",
    "0.462184761415\n",
    "0.462184761415\n",
    "0.483014077264\n",
    "0.370363823359\n",
    "0.395149021877\n",
    "0.382065415354\n",
    "0.376743588095\n",
    "0.368071179105\n",
    "0.368566668809\n",
    "0.370725396927..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53729148393\n",
      "0.754651751699\n",
      "0.662880610765\n",
      "-0.0956428657932\n",
      "0.808791103723\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "appear = df.rdd.map(lambda x: np.array(x['appearance']))\n",
    "arom = df.rdd.map(lambda x: np.array(x['aroma']))   \n",
    "pal = df.rdd.map(lambda x: np.array(x['palate']))          \n",
    "style = df.rdd.map(lambda x: np.array(x['style']))\n",
    "taste = df.rdd.map(lambda x: np.array(x['taste']))\n",
    "overall = df.rdd.map(lambda x: np.array(x['overall']))\n",
    "\n",
    "app_corr = Statistics.corr(appear,overall)\n",
    "arom_corr = Statistics.corr(arom,overall)\n",
    "pal_corr = Statistics.corr(pal,overall)\n",
    "style_corr = Statistics.corr(style,overall)\n",
    "taste_corr = Statistics.corr(taste,overall)\n",
    "\n",
    "print app_corr\n",
    "print arom_corr\n",
    "print pal_corr\n",
    "print style_corr\n",
    "print taste_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774703758192\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def getFeatures3(review):\n",
    "    # normalize text and separate by spaces\n",
    "    words = review.lower().encode('utf-8').translate(string.maketrans(\"\",\"\"), \\\n",
    "                                                     string.punctuation).split()\n",
    "    # obtain word_ids for vocab words in the review\n",
    "    word_ids = []\n",
    "    for i,word in enumerate(words):\n",
    "        if d.has_key(word):\n",
    "            word_ids.append(d[word])\n",
    "    vals = [1]*len(word_ids)\n",
    "    rows = [0]*len(vals)\n",
    "    \n",
    "    # create a sparse matrix feature vector\n",
    "    result = coo_matrix((vals, (rows,word_ids)), shape=(1,len(vocab))).toarray()[0]\n",
    "    return result\n",
    "\n",
    "training = train.map(lambda x: LabeledPoint(int(x['overall'] >= 14),np.concatenate(([x['aroma'], x['palate'], x['taste']],getFeatures3(x['review'])), axis=0)))\n",
    "testing = test.map(lambda x: LabeledPoint(int(x['overall'] >= 14),np.concatenate(([x['aroma'], x['palate'], x['taste']],getFeatures3(x['review'])), axis=0)))\n",
    "\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "predictionAndLabel = testing.map(lambda p: (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we looked at the correlations between each of the 5 scored features with the overall score. We found that appearance and style are the mostly weakly correlated and thus did not include them as features. As features, we used scores for aroma, palate, and taste as well as the text review in the same feature vector form as part 1. A Naive Bayes classifier was trained using the NaiveBayesModel from the pyspark machine learning library. On a sample test set, we were able to obtain ~77.5% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_unlabeled = spark.read.json(\"/project/cmsc25025/beer_review/unlabeled.json\")\n",
    "\n",
    "unlabeled = df_unlabeled.rdd.map(lambda x: (x['review_id'],np.concatenate(([x['aroma'], x['palate'], x['taste']],getFeatures3(x['review'])), axis=0)))\n",
    "predict = unlabeled.map(lambda p: (p[0],model.predict(p[1]))).collectAsMap()\n",
    "\n",
    "with open(\"assn3_prob4_predict.json\", \"w\") as f:\n",
    "    json.dump(predict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
